import html
import re
from datetime import date, timedelta
from textwrap import dedent
from typing import Optional

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st

st.set_page_config(
    page_title="CRMíŒ€ ê¸°íšì „ ì„±ê³¼ ë¶„ì„",
    layout="wide",
    page_icon="Symbol.png",
)

st.markdown(
    """
<style>
div[data-testid="stDownloadButton"] > button {
  background: linear-gradient(90deg, #7c3aed 0%, #ec4899 100%) !important;
  color: #ffffff !important;
  border: 0 !important;
  border-radius: 12px !important;
  padding: 0.7rem 1.1rem !important;
  font-weight: 700 !important;
}
div[data-testid="stDownloadButton"] > button:hover {
  filter: brightness(1.05);
  transform: translateY(-1px);
}
div[data-testid="stDownloadButton"] > button:active {
  transform: translateY(0px);
}
</style>
""",
    unsafe_allow_html=True,
)

def safe_concat(frames: list[pd.DataFrame]) -> pd.DataFrame:
    non_empty = [df for df in frames if df is not None and not df.empty]
    if not non_empty:
        return pd.DataFrame()
    return pd.concat(non_empty, ignore_index=True)


def _seek_start(file) -> None:
    try:
        file.seek(0)
    except Exception:
        pass


def _resolve_rename_and_usecols(
    header_cols: list[str],
    required_columns: list[str],
    aliases: dict[str, list[str]],
) -> tuple[dict[str, str], list[str]]:
    rename_map: dict[str, str] = {}
    usecols: list[str] = []
    for required in required_columns:
        candidates = aliases.get(required, [required])
        matched = next((c for c in candidates if c in header_cols), None)
        if matched is None:
            continue
        rename_map[matched] = required
        usecols.append(matched)
    return rename_map, usecols


REQUIRED_COLUMNS = [
    "ë³‘ì› ID",
    "ë³‘ì› ì´ë¦„",
    "ëŒ€í–‰ì‚¬ ID",
    "ëŒ€í–‰ì‚¬ ì´ë¦„",
    "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)",
    "ì´ë²¤íŠ¸ ì´ë¦„",
    "ì´ë²¤íŠ¸ ê°€ê²© (text)",
    "ì¹´í…Œê³ ë¦¬ (ìµœìƒìœ„)",
    "ì¹´í…Œê³ ë¦¬ (ëŒ€)",
    "ì¹´í…Œê³ ë¦¬ (ì¤‘)",
    "ì¹´í…Œê³ ë¦¬ (ì†Œ)",
    "ëŒ€ìƒì¼",
    "ì¡°íšŒ ìˆ˜",
    "ìƒë‹´ì‹ ì²­ ìˆ˜",
]

COLUMN_ALIASES = {
    "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)": ["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ì´ë²¤íŠ¸ ID"],
    "ë³‘ì› ì´ë¦„": ["ë³‘ì› ì´ë¦„", "ë³‘ì›ëª…"],
}

CPV_REQUIRED_COLUMNS = [
    "ë³‘ì› ID",
    "ë³‘ì› ì´ë¦„",
    "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)",
    "ì´ë²¤íŠ¸ ì´ë¦„",
    "ëŒ€ì¹´í…Œê³ ë¦¬ëª…",
    "ì¤‘ì¹´í…Œê³ ë¦¬ëª…",
    "ì†Œì¹´í…Œê³ ë¦¬ëª…",
    "ì´ë²¤íŠ¸ í• ì¸ê°€",
    "ëŒ€ìƒì¼",
    "CPV ì¡°íšŒ ìˆ˜",
    "CPV ë§¤ì¶œ",
]

CPV_COLUMN_ALIASES = {
    "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)": ["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ì´ë²¤íŠ¸ ID"],
    "ë³‘ì› ì´ë¦„": ["ë³‘ì› ì´ë¦„", "ë³‘ì›ëª…"],
    "ì´ë²¤íŠ¸ ì´ë¦„": ["ì´ë²¤íŠ¸ ì´ë¦„", "ì´ë²¤íŠ¸ëª…"],
    "ëŒ€ì¹´í…Œê³ ë¦¬ëª…": ["ëŒ€ì¹´í…Œê³ ë¦¬ëª…"],
    "ì¤‘ì¹´í…Œê³ ë¦¬ëª…": ["ì¤‘ì¹´í…Œê³ ë¦¬ëª…"],
    "ì†Œì¹´í…Œê³ ë¦¬ëª…": ["ì†Œì¹´í…Œê³ ë¦¬ëª…"],
    "ì´ë²¤íŠ¸ í• ì¸ê°€": ["ì´ë²¤íŠ¸ í• ì¸ê°€"],
    "CPV ì¡°íšŒ ìˆ˜": ["CPV ì¡°íšŒ ìˆ˜"],
    "CPV ë§¤ì¶œ": ["CPV ë§¤ì¶œ"],
}


def normalize_columns(
    df: pd.DataFrame,
    required_columns: list[str],
    aliases: dict[str, list[str]],
) -> pd.DataFrame:
    rename_map = {}
    missing = []
    for required in required_columns:
        candidates = aliases.get(required, [required])
        matched = next((c for c in candidates if c in df.columns), None)
        if matched:
            rename_map[matched] = required
        else:
            missing.append(required)
    if missing:
        raise ValueError(f"ë‹¤ìŒ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {', '.join(missing)}")
    return df.rename(columns=rename_map)


@st.cache_data(show_spinner=True)
def load_data(file) -> pd.DataFrame:
    df = pd.read_csv(file, encoding="utf-8-sig", low_memory=False)
    df = normalize_columns(df, REQUIRED_COLUMNS, COLUMN_ALIASES)

    df["ëŒ€ìƒì¼"] = pd.to_datetime(df["ëŒ€ìƒì¼"], errors="coerce")
    if df["ëŒ€ìƒì¼"].isna().any():
        raise ValueError("ëŒ€ìƒì¼ ì»¬ëŸ¼ì— ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê°’ì´ ìˆìŠµë‹ˆë‹¤. YYYY-MM-DD í˜•ì‹ì„ í™•ì¸í•˜ì„¸ìš”.")

    for metric_col in ["ì¡°íšŒ ìˆ˜", "ìƒë‹´ì‹ ì²­ ìˆ˜"]:
        df[metric_col] = (
            pd.to_numeric(df[metric_col], errors="coerce")
            .fillna(0)
            .astype(int)
        )
    df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] = df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"].astype(str)
    return df


def _clean_text(value: str) -> str:
    if value is None:
        return ""
    if pd.isna(value):
        return ""
    return str(value).replace('"', "").strip()


@st.cache_data(show_spinner=True)
def load_primary_data(file) -> pd.DataFrame:
    try:
        file.seek(0)
    except Exception:
        pass
    try:
        return load_data(file)
    except Exception:
        pass

    try:
        file.seek(0)
    except Exception:
        pass
    df = pd.read_csv(file, encoding="utf-8-sig", skiprows=3)
    df.columns = [_clean_text(c) for c in df.columns]

    def find_column(candidates: list[str]) -> Optional[str]:
        candidates_lower = {c.lower(): c for c in df.columns}
        for cand in candidates:
            if cand in df.columns:
                return cand
            lc = cand.lower()
            if lc in candidates_lower:
                return candidates_lower[lc]
        return None

    event_id_col = find_column(["event_id", "ì´ë²¤íŠ¸ ID", "ì´ë²¤íŠ¸ID"])
    date_col = find_column(["Time", "time", "date", "day", "ëŒ€ìƒì¼", "ì¼ì", "ë‚ ì§œ"])
    view_col = find_column(["pageview_event.detail--All Users"])
    apply_col = find_column(["apply_event--All Users"])
    event_name_col = find_column(["event_name", "ì´ë²¤íŠ¸ëª…", "ì´ë²¤íŠ¸ ì´ë¦„"])

    missing = []
    if event_id_col is None:
        missing.append("event_id")
    if date_col is None:
        missing.append("date")
    if view_col is None:
        missing.append("pageview_event.detail--All Users")
    if apply_col is None:
        missing.append("apply_event--All Users")
    if missing:
        raise ValueError(
            "ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¡°íšŒ/ìƒë‹´ CSV í˜•ì‹ì…ë‹ˆë‹¤. ëˆ„ë½ ì»¬ëŸ¼: " + ", ".join(missing)
        )

    out = pd.DataFrame(index=df.index, columns=REQUIRED_COLUMNS)
    out["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] = df[event_id_col].map(_clean_text).astype(str)
    if event_name_col is not None:
        out["ì´ë²¤íŠ¸ ì´ë¦„"] = df[event_name_col].map(_clean_text)
    out["ëŒ€ìƒì¼"] = pd.to_datetime(df[date_col].map(_clean_text), errors="coerce")
    if out["ëŒ€ìƒì¼"].isna().any():
        raise ValueError("ì¡°íšŒ/ìƒë‹´ CSVì˜ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    out["ì¡°íšŒ ìˆ˜"] = pd.to_numeric(df[view_col], errors="coerce").fillna(0).astype(int)
    out["ìƒë‹´ì‹ ì²­ ìˆ˜"] = (
        pd.to_numeric(df[apply_col], errors="coerce").fillna(0).astype(int)
    )
    return out


@st.cache_data(show_spinner=False)
def load_primary_meta(file) -> pd.DataFrame:
    # Try existing format meta read (header row is the first row)
    _seek_start(file)
    try:
        header = pd.read_csv(file, encoding="utf-8-sig", nrows=0).columns.tolist()
        meta_required = ["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ì´ë²¤íŠ¸ ì´ë¦„", "ëŒ€ìƒì¼", "ë³‘ì› ì´ë¦„"]
        rename_map, usecols = _resolve_rename_and_usecols(
            header, meta_required, COLUMN_ALIASES
        )
        if "ëŒ€ìƒì¼" not in rename_map.values() or "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)" not in rename_map.values():
            raise ValueError("not_primary_format")
        _seek_start(file)
        df = pd.read_csv(
            file,
            encoding="utf-8-sig",
            usecols=usecols,
            low_memory=False,
        ).rename(columns=rename_map)
        df["ëŒ€ìƒì¼"] = pd.to_datetime(df["ëŒ€ìƒì¼"], errors="coerce")
        df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] = df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"].astype(str)
        return df
    except Exception:
        pass

    # Amplitude format meta read (header row is the 4th row)
    _seek_start(file)
    df = pd.read_csv(file, encoding="utf-8-sig", skiprows=3, nrows=0)
    raw_header = df.columns.tolist()
    clean_to_raw: dict[str, str] = {}
    header: list[str] = []
    for raw in raw_header:
        clean = _clean_text(raw)
        header.append(clean)
        if clean and clean not in clean_to_raw:
            clean_to_raw[clean] = raw

    def find_column(candidates: list[str]) -> tuple[Optional[str], Optional[str]]:
        candidates_lower = {c.lower(): c for c in header if c}
        for cand in candidates:
            if cand in header:
                return cand, clean_to_raw.get(cand, cand)
            lc = cand.lower()
            if lc in candidates_lower:
                clean = candidates_lower[lc]
                return clean, clean_to_raw.get(clean, clean)
        return None, None

    event_id_col, event_id_raw = find_column(["event_id", "ì´ë²¤íŠ¸ ID", "ì´ë²¤íŠ¸ID"])
    date_col, date_raw = find_column(["Time", "time", "date", "day", "ëŒ€ìƒì¼", "ì¼ì", "ë‚ ì§œ"])
    event_name_col, event_name_raw = find_column(["event_name", "ì´ë²¤íŠ¸ëª…", "ì´ë²¤íŠ¸ ì´ë¦„"])
    if event_id_col is None or date_col is None or event_id_raw is None or date_raw is None:
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¡°íšŒ/ìƒë‹´ CSV í˜•ì‹ì…ë‹ˆë‹¤.")

    usecols = [c for c in [event_id_raw, date_raw, event_name_raw] if c is not None]
    _seek_start(file)
    raw = pd.read_csv(file, encoding="utf-8-sig", skiprows=3, usecols=usecols)
    raw.columns = [_clean_text(c) for c in raw.columns]

    meta = pd.DataFrame(index=raw.index, columns=["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ì´ë²¤íŠ¸ ì´ë¦„", "ëŒ€ìƒì¼", "ë³‘ì› ì´ë¦„"])
    meta["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] = raw[event_id_col].map(_clean_text).astype(str)
    if event_name_col is not None and event_name_col in raw.columns:
        meta["ì´ë²¤íŠ¸ ì´ë¦„"] = raw[event_name_col].map(_clean_text)
    meta["ëŒ€ìƒì¼"] = pd.to_datetime(raw[date_col].map(_clean_text), errors="coerce")
    return meta


@st.cache_data(show_spinner=False)
def load_cpv_meta(file) -> pd.DataFrame:
    _seek_start(file)
    header = pd.read_csv(file, encoding="utf-8-sig", nrows=0).columns.tolist()
    meta_required = [
        "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)",
        "ì´ë²¤íŠ¸ ì´ë¦„",
        "ëŒ€ìƒì¼",
        "ë³‘ì› ì´ë¦„",
        "ëŒ€ì¹´í…Œê³ ë¦¬ëª…",
        "ì¤‘ì¹´í…Œê³ ë¦¬ëª…",
        "ì†Œì¹´í…Œê³ ë¦¬ëª…",
        "ì´ë²¤íŠ¸ í• ì¸ê°€",
    ]
    rename_map, usecols = _resolve_rename_and_usecols(
        header, meta_required, CPV_COLUMN_ALIASES
    )
    if "ëŒ€ìƒì¼" not in rename_map.values() or "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)" not in rename_map.values():
        raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” CPV CSV í˜•ì‹ì…ë‹ˆë‹¤.")
    _seek_start(file)
    df = pd.read_csv(
        file,
        encoding="utf-8-sig",
        usecols=usecols,
        low_memory=False,
    ).rename(columns=rename_map)
    df["ëŒ€ìƒì¼"] = pd.to_datetime(df["ëŒ€ìƒì¼"], errors="coerce")
    df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] = df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"].astype(str)
    return df


@st.cache_data(show_spinner=True)
def load_cpv_data(file) -> pd.DataFrame:
    df = pd.read_csv(file, encoding="utf-8-sig", low_memory=False)
    df = normalize_columns(df, CPV_REQUIRED_COLUMNS, CPV_COLUMN_ALIASES)
    df["ëŒ€ìƒì¼"] = pd.to_datetime(df["ëŒ€ìƒì¼"], errors="coerce")
    if df["ëŒ€ìƒì¼"].isna().any():
        raise ValueError("CPV ë°ì´í„°ì˜ ëŒ€ìƒì¼ ì»¬ëŸ¼ì— ë³€í™˜í•  ìˆ˜ ì—†ëŠ” ê°’ì´ ìˆìŠµë‹ˆë‹¤.")
    for metric_col in ["CPV ì¡°íšŒ ìˆ˜", "CPV ë§¤ì¶œ", "ì´ë²¤íŠ¸ í• ì¸ê°€"]:
        if metric_col not in df.columns:
            continue
        df[metric_col] = (
            pd.to_numeric(df[metric_col], errors="coerce").fillna(0).astype(int)
        )
    df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] = df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"].astype(str)
    return df


def get_event_options(df: pd.DataFrame):
    options = (
        df[["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ì´ë²¤íŠ¸ ì´ë¦„"]]
        .drop_duplicates()
        .sort_values("ì´ë²¤íŠ¸ ì´ë¦„")
    )
    return list(options.itertuples(index=False, name=None))


def parse_event_ids_input(raw_text: str, valid_ids: set[str]):
    if not raw_text:
        return [], []
    tokens = [
        token.strip()
        for token in re.split(r"[,\n]+", raw_text)
        if token.strip()
    ]
    seen = []
    for token in tokens:
        if token not in seen:
            seen.append(token)
    invalid = [token for token in seen if token not in valid_ids]
    valid = [token for token in seen if token in valid_ids]
    return valid, invalid


def render_selected_events(selected_ids: list[str], event_lookup: dict[str, str]):
    if not selected_ids:
        st.info("ì„ íƒëœ ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    data = [
        {"ì´ë²¤íŠ¸ ID": event_id, "ì´ë²¤íŠ¸ ì´ë¦„": event_lookup.get(event_id, "ì•Œ ìˆ˜ ì—†ìŒ")}
        for event_id in selected_ids
    ]
    df = pd.DataFrame(data)
    with st.expander("ì„ íƒëœ ì´ë²¤íŠ¸ ëª©ë¡", expanded=False):
        st.dataframe(df, hide_index=True, width="stretch", height=240)


def get_date_range_input(df: pd.DataFrame):
    min_date = df["ëŒ€ìƒì¼"].min().date()
    max_date = df["ëŒ€ìƒì¼"].max().date()
    default_range = (min_date, max_date)
    selected = st.sidebar.date_input(
        "ë¶„ì„ ê¸°ê°„ (ì§„í–‰ ê¸°ê°„)",
        value=default_range,
        min_value=min_date,
        max_value=max_date,
    )
    if isinstance(selected, date):
        return selected, selected
    if isinstance(selected, (list, tuple)) and len(selected) == 2:
        return selected[0], selected[1]
    st.sidebar.error("ê¸°ê°„ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì„ ëª¨ë‘ ì„ íƒí•´ì£¼ì„¸ìš”.")
    st.stop()


def filter_period(df: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp):
    mask = (df["ëŒ€ìƒì¼"] >= start) & (df["ëŒ€ìƒì¼"] <= end)
    return df.loc[mask].copy()


def build_event_summary(
    current_df: pd.DataFrame,
    previous_df: pd.DataFrame,
    cpv_current_df: pd.DataFrame,
    cpv_previous_df: pd.DataFrame,
    event_lookup: dict[str, str],
) -> pd.DataFrame:
    metrics = ["ì¡°íšŒ ìˆ˜", "ìƒë‹´ì‹ ì²­ ìˆ˜"]
    hospital_sources = [
        current_df[["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ë³‘ì› ì´ë¦„"]],
        previous_df[["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ë³‘ì› ì´ë¦„"]],
        cpv_current_df[["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ë³‘ì› ì´ë¦„"]],
        cpv_previous_df[["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ë³‘ì› ì´ë¦„"]],
    ]
    hospital_info = (
        pd.concat(hospital_sources, ignore_index=True)
        .dropna(subset=["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"])
        .drop_duplicates(subset=["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"])
    )
    id_sources = [
        current_df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"],
        previous_df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"],
        cpv_current_df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"],
        cpv_previous_df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"],
    ]
    combined_ids = (
        pd.concat(id_sources, ignore_index=True)
        .dropna()
        .astype(str)
        .unique()
    )
    summary = pd.DataFrame({"ì´ë²¤íŠ¸ ID (ì‹ë³„ì)": combined_ids})
    if summary.empty:
        return summary

    current = (
        current_df.groupby("ì´ë²¤íŠ¸ ID (ì‹ë³„ì)")[metrics]
        .sum()
        .reset_index()
        .rename(
            columns={
                metric: f"{metric} (ì§„í–‰ ê¸°ê°„)" for metric in metrics
            }
        )
    )
    previous = (
        previous_df.groupby("ì´ë²¤íŠ¸ ID (ì‹ë³„ì)")[metrics]
        .sum()
        .reset_index()
        .rename(
            columns={
                metric: f"{metric} (ì´ì „ ê¸°ê°„)" for metric in metrics
            }
        )
    )
    summary = summary.merge(current, on="ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", how="left")
    summary = summary.merge(previous, on="ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", how="left")
    summary["ì´ë²¤íŠ¸ ì´ë¦„"] = summary["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"].map(event_lookup)
    summary = summary.merge(hospital_info, on="ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", how="left")

    cpv_metrics = ["CPV ì¡°íšŒ ìˆ˜", "CPV ë§¤ì¶œ"]
    cpv_attrs = [
        "ë³‘ì› ì´ë¦„",
        "ëŒ€ì¹´í…Œê³ ë¦¬ëª…",
        "ì¤‘ì¹´í…Œê³ ë¦¬ëª…",
        "ì†Œì¹´í…Œê³ ë¦¬ëª…",
        "ì´ë²¤íŠ¸ í• ì¸ê°€",
        "ì´ë²¤íŠ¸ ì´ë¦„",
    ]
    cpv_attr_df = (
        pd.concat([cpv_current_df[["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] + cpv_attrs], cpv_previous_df[["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] + cpv_attrs]], ignore_index=True)
        if (not cpv_current_df.empty or not cpv_previous_df.empty)
        else pd.DataFrame(columns=["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"] + cpv_attrs)
    ).drop_duplicates(subset=["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"])
    if not cpv_attr_df.empty:
        summary = summary.merge(
            cpv_attr_df,
            on="ì´ë²¤íŠ¸ ID (ì‹ë³„ì)",
            how="left",
            suffixes=("", "_cpv"),
        )
        if "ë³‘ì› ì´ë¦„_cpv" in summary.columns:
            summary["ë³‘ì› ì´ë¦„"] = summary["ë³‘ì› ì´ë¦„_cpv"].combine_first(summary["ë³‘ì› ì´ë¦„"])
        if "ì´ë²¤íŠ¸ ì´ë¦„_cpv" in summary.columns:
            summary["ì´ë²¤íŠ¸ ì´ë¦„"] = summary["ì´ë²¤íŠ¸ ì´ë¦„_cpv"].combine_first(summary["ì´ë²¤íŠ¸ ì´ë¦„"])
        for extra in ["ë³‘ì› ì´ë¦„_cpv", "ì´ë²¤íŠ¸ ì´ë¦„_cpv"]:
            if extra in summary.columns:
                summary = summary.drop(columns=[extra])
    cpv_current = (
        cpv_current_df.groupby("ì´ë²¤íŠ¸ ID (ì‹ë³„ì)")[cpv_metrics]
        .sum()
        .reset_index()
        .rename(columns={metric: f"{metric} (ì§„í–‰ ê¸°ê°„)" for metric in cpv_metrics})
    )
    cpv_previous = (
        cpv_previous_df.groupby("ì´ë²¤íŠ¸ ID (ì‹ë³„ì)")[cpv_metrics]
        .sum()
        .reset_index()
        .rename(columns={metric: f"{metric} (ì´ì „ ê¸°ê°„)" for metric in cpv_metrics})
    )
    summary = summary.merge(cpv_current, on="ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", how="left")
    summary = summary.merge(cpv_previous, on="ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", how="left")

    all_metrics = metrics + cpv_metrics
    for metric in all_metrics:
        for period in ["ì§„í–‰ ê¸°ê°„", "ì´ì „ ê¸°ê°„"]:
            col = f"{metric} ({period})"
            if col not in summary:
                summary[col] = 0
            else:
                summary[col] = summary[col].fillna(0)
        current_col = f"{metric} (ì§„í–‰ ê¸°ê°„)"
        previous_col = f"{metric} (ì´ì „ ê¸°ê°„)"
        diff_col = f"{metric} ì¦ê°ëŸ‰"
        summary[diff_col] = summary[current_col] - summary[previous_col]
        rate_col = f"{metric} ì¦ê°ë¥ "
        summary[rate_col] = np.where(
            summary[previous_col] > 0,
            summary[diff_col] / summary[previous_col] * 100,
            np.nan,
        )

    columns_order = [
        "ì´ë²¤íŠ¸ ì´ë¦„",
        "ë³‘ì› ì´ë¦„",
        "ëŒ€ì¹´í…Œê³ ë¦¬ëª…",
        "ì¤‘ì¹´í…Œê³ ë¦¬ëª…",
        "ì†Œì¹´í…Œê³ ë¦¬ëª…",
        "ì´ë²¤íŠ¸ í• ì¸ê°€",
        "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)",
        "ì¡°íšŒ ìˆ˜ (ì§„í–‰ ê¸°ê°„)",
        "ì¡°íšŒ ìˆ˜ (ì´ì „ ê¸°ê°„)",
        "ì¡°íšŒ ìˆ˜ ì¦ê°ëŸ‰",
        "ì¡°íšŒ ìˆ˜ ì¦ê°ë¥ ",
        "ìƒë‹´ì‹ ì²­ ìˆ˜ (ì§„í–‰ ê¸°ê°„)",
        "ìƒë‹´ì‹ ì²­ ìˆ˜ (ì´ì „ ê¸°ê°„)",
        "ìƒë‹´ì‹ ì²­ ìˆ˜ ì¦ê°ëŸ‰",
        "ìƒë‹´ì‹ ì²­ ìˆ˜ ì¦ê°ë¥ ",
        "CPV ë§¤ì¶œ (ì§„í–‰ ê¸°ê°„)",
        "CPV ë§¤ì¶œ (ì´ì „ ê¸°ê°„)",
        "CPV ë§¤ì¶œ ì¦ê°ëŸ‰",
        "CPV ë§¤ì¶œ ì¦ê°ë¥ ",
    ]
    existing_columns = [col for col in columns_order if col in summary.columns]
    return summary[existing_columns].sort_values(
        "ìƒë‹´ì‹ ì²­ ìˆ˜ (ì§„í–‰ ê¸°ê°„)", ascending=False
    )


def generate_event_insights(summary_df: pd.DataFrame, top_n: int = 3) -> list[dict]:
    if summary_df.empty:
        return [
            {
                "title": "ë°ì´í„° ë¶€ì¡±",
                "badge": "Info",
                "items": ["ì„ íƒëœ ì´ë²¤íŠ¸ì— ëŒ€í•œ ë¹„êµ ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."],
            }
        ]

    def format_label(row: pd.Series) -> str:
        event_name = row.get("ì´ë²¤íŠ¸ ì´ë¦„") or row.get("ì´ë²¤íŠ¸ ID (ì‹ë³„ì)") or ""
        hospital_name = row.get("ë³‘ì› ì´ë¦„")
        if pd.notna(hospital_name) and str(hospital_name).strip():
            return f"{event_name} ({hospital_name})"
        return str(event_name)

    def add_amount_card(metric: str):
        diff_col = f"{metric} ì¦ê°ëŸ‰"
        if diff_col not in summary_df:
            return
        positive = summary_df[summary_df[diff_col] > 0]
        if positive.empty:
            return
        top_positive = positive.sort_values(diff_col, ascending=False).head(top_n)
        unit = "ì›" if metric == "CPV ë§¤ì¶œ" else "ê±´"
        items = [
            {
                "label": format_label(row),
                "value": f"+{int(row[diff_col]):,}{unit}",
            }
            for _, row in top_positive.iterrows()
        ]
        insights.append(
            {
                "title": f"{metric} ìƒìŠ¹ TOP {len(items)}",
                "badge": metric,
                "items": items,
            }
        )

    def add_rate_card(metric: str):
        diff_rate_col = f"{metric} ì¦ê°ë¥ "
        diff_amount_col = f"{metric} ì¦ê°ëŸ‰"
        if diff_rate_col not in summary_df:
            return
        positive_rate = summary_df[summary_df[diff_rate_col] > 0]
        if positive_rate.empty:
            return
        top_rate = positive_rate.sort_values(diff_rate_col, ascending=False).head(top_n)
        unit = "ì›" if metric == "CPV ë§¤ì¶œ" else "ê±´"
        items = []
        for _, row in top_rate.iterrows():
            amount = row.get(diff_amount_col)
            amount_part = (
                f"+{int(amount):,}{unit}" if pd.notna(amount) and amount != 0 else "-"
            )
            items.append(
                {
                    "label": format_label(row),
                    "value": f"+{row[diff_rate_col]:.1f}% ({amount_part})",
                }
            )
        insights.append(
            {
                "title": f"{metric} ì¦ê°ë¥  TOP {len(items)}",
                "badge": f"{metric} ì¦ê°ë¥ ",
                "items": items,
            }
        )

    insights: list[dict] = []
    add_amount_card("ìƒë‹´ì‹ ì²­ ìˆ˜")
    add_rate_card("ìƒë‹´ì‹ ì²­ ìˆ˜")
    add_amount_card("ì¡°íšŒ ìˆ˜")
    add_rate_card("ì¡°íšŒ ìˆ˜")
    add_amount_card("CPV ë§¤ì¶œ")
    add_rate_card("CPV ë§¤ì¶œ")

    if not insights:
        insights.append(
            {
                "title": "ë³€í™” ì—†ìŒ",
                "badge": "Info",
                "items": ["ë‘ ê¸°ê°„ ì‚¬ì´ì—ì„œ ëšœë ·í•œ ì¦ê°ì„ ë³´ì´ëŠ” ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."],
            }
        )

    total_events = int(len(summary_df))
    if total_events > 0:
        views_up = int((summary_df.get("ì¡°íšŒ ìˆ˜ ì¦ê°ëŸ‰", 0) > 0).sum())
        applies_up = int((summary_df.get("ìƒë‹´ì‹ ì²­ ìˆ˜ ì¦ê°ëŸ‰", 0) > 0).sum())
        views_pct = (views_up / total_events) * 100
        applies_pct = (applies_up / total_events) * 100
        insights.append(
            {
                "title": "ê¸°íšì „ ë‚´ ì´ë²¤íŠ¸ ì„±ì¥ í˜„í™©",
                "badge": "ìš”ì•½",
                "items": [
                    {"label": "ì „ì²´ ì´ë²¤íŠ¸ ìˆ˜", "value": f"{total_events:,}"},
                    {
                        "label": "ì¡°íšŒìˆ˜ ì¦ê°€ ì´ë²¤íŠ¸",
                        "value": f"{views_up:,} ({views_pct:.1f}%)",
                    },
                    {
                        "label": "ìƒë‹´ì‹ ì²­ ì¦ê°€ ì´ë²¤íŠ¸",
                        "value": f"{applies_up:,} ({applies_pct:.1f}%)",
                    },
                ],
            }
        )
    return insights


def format_event_summary_display(summary_df: pd.DataFrame) -> pd.DataFrame:
    display_df = summary_df.copy()
    number_cols = [
        col
        for col in display_df.columns
        if (
            (
                "ì¡°íšŒ ìˆ˜" in col
                or "ìƒë‹´ì‹ ì²­ ìˆ˜" in col
                or "CPV ë§¤ì¶œ" in col
                or col == "ì´ë²¤íŠ¸ í• ì¸ê°€"
            )
            and "ì¦ê°ë¥ " not in col
        )
    ]
    rate_cols = [col for col in display_df.columns if "ì¦ê°ë¥ " in col]

    for col in number_cols:
        display_df[col] = display_df[col].apply(
            lambda x: f"{int(x):,}" if pd.notna(x) and x != 0 else "-"
        )
    for col in rate_cols:
        display_df[col] = display_df[col].apply(
            lambda x: f"{x:+.1f}%" if pd.notna(x) else "-"
        )
    return display_df


def render_insight_cards(insights: list[dict]):
    if not insights:
        st.info("í‘œì‹œí•  ì¸ì‚¬ì´íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    columns_per_row = min(2, len(insights))
    card_template = dedent(
        """
        <div style="
            border-radius: 12px;
            padding: 16px;
            margin-bottom: 12px;
            background: linear-gradient(135deg, #eef2ff, #fef3c7);
            border: 1px solid #e5e7eb;
            box-shadow: 0 8px 20px rgba(15, 23, 42, 0.08);
        ">
            <div style="font-size:12px;font-weight:600;color:#6366f1;">{badge}</div>
            <div style="font-size:16px;font-weight:700;color:#111827;margin-top:4px;">{title}</div>
            <div style="margin-top:10px;">{items}</div>
        </div>
        """
    ).strip()
    item_template = dedent(
        """
        <div style="
            display:flex;
            justify-content:space-between;
            font-size:14px;
            color:#374151;
            padding:4px 0;
            border-bottom:1px dashed #e5e7eb;
        ">
            <span style="flex:1; margin-right:8px;">{label}</span>
            <span style="font-weight:600;color:#111827;">{value}</span>
        </div>
        """
    ).strip()
    for start in range(0, len(insights), columns_per_row):
        cols = st.columns(columns_per_row)
        row_insights = insights[start : start + columns_per_row]
        for idx, insight in enumerate(row_insights):
            items = insight.get("items") or []
            rendered_items = []
            for item in items:
                if isinstance(item, dict):
                    label = item.get("label", "")
                    value = item.get("value", "")
                elif isinstance(item, str):
                    label, value = item, ""
                else:
                    label, value = str(item), ""
                label_safe = html.escape(str(label))
                value_safe = html.escape(str(value))
                rendered_items.append(
                    item_template.format(label=label_safe, value=value_safe)
                )
            badge_safe = html.escape(str(insight.get("badge", "")))
            title_safe = html.escape(str(insight.get("title", "")))
            with cols[idx]:
                st.markdown(
                    card_template.format(
                        badge=badge_safe,
                        title=title_safe,
                        items="".join(rendered_items) or "<div>ë°ì´í„° ì—†ìŒ</div>",
                    ),
                    unsafe_allow_html=True,
                )


def build_timeseries_with_dates(df: pd.DataFrame, label: str):
    if df.empty:
        return pd.DataFrame(columns=["Day", "Date", "ìƒë‹´ì‹ ì²­ ìˆ˜", "ê¸°ê°„"])
    sorted_df = df.sort_values("ëŒ€ìƒì¼")
    day_offsets = (
        sorted_df["ëŒ€ìƒì¼"].values - sorted_df["ëŒ€ìƒì¼"].min().to_datetime64()
    ) / np.timedelta64(1, "D")
    sorted_df["Day"] = day_offsets.astype(int) + 1
    sorted_df["Date"] = sorted_df["ëŒ€ìƒì¼"].dt.date
    ts = (
        sorted_df.groupby(["Day", "Date"])["ìƒë‹´ì‹ ì²­ ìˆ˜"]
        .sum()
        .reset_index()
    )
    ts["ê¸°ê°„"] = label
    return ts


def render_metrics(metric_rows: list[dict]):
    if not metric_rows:
        return
    cols = st.columns(len(metric_rows))
    for idx, metric in enumerate(metric_rows):
        label = metric["label"]
        current_raw = metric.get("current")
        previous_raw = metric.get("previous")

        current_display = "-"
        delta = None

        if current_raw is not None:
            current_val = int(current_raw)
            current_display = f"{current_val:,}"

        if current_raw is not None and previous_raw is not None:
            previous_val = int(previous_raw)
            delta_numeric = current_val - previous_val
            if previous_val == 0:
                delta = f"{delta_numeric:+,} (ì´ì „ ê¸°ê°„ 0)"
            else:
                delta_percentage = (delta_numeric / previous_val) * 100
                delta = f"{delta_numeric:+,} ({delta_percentage:+.1f}%)"

        cols[idx].metric(
            label,
            current_display,
            delta=delta or "ë¹„êµ ë°ì´í„° ì—†ìŒ",
        )


def render_chart(current_df: pd.DataFrame):
    chart_df = build_timeseries_with_dates(current_df, "ì§„í–‰ ê¸°ê°„")
    if chart_df.empty:
        st.info("ì°¨íŠ¸ë¥¼ í‘œì‹œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    chart_df["DayLabel"] = chart_df.apply(
        lambda row: f"Day {int(row['Day'])} ({row['Date']})", axis=1
    )
    fig = px.line(
        chart_df,
        x="DayLabel",
        y="ìƒë‹´ì‹ ì²­ ìˆ˜",
        markers=True,
        labels={"DayLabel": "ê²½ê³¼ ì¼ìˆ˜ (ë‚ ì§œ)", "ìƒë‹´ì‹ ì²­ ìˆ˜": "ìƒë‹´ì‹ ì²­ ìˆ˜"},
    )
    fig.update_layout(height=400, hovermode="x unified", showlegend=False)
    st.plotly_chart(fig, width="stretch")


st.title("ğŸ’œ CRMíŒ€ ê¸°íšì „ ì„±ê³¼ ë¶„ì„")
st.markdown(
    """
ì¡°íšŒ/ìƒë‹´ CSVëŠ” ì•°í”Œë¦¬íŠœë“œ([ë§í¬](https://app.amplitude.com/analytics/babitalk/chart/g7sowyhf))ì—ì„œ, \nCPV CSVëŠ” í€µì‚¬ì´íŠ¸([ë§í¬](https://ap-northeast-2.quicksight.aws.amazon.com/sn/account/babitalk-data-quicksight/dashboards/74afc507-059e-421c-910d-303f57ae1900/sheets/74afc507-059e-421c-910d-303f57ae1900_01e8ddcf-7b44-4d69-8cd4-7eb22915f9ec))ì—ì„œ ë‹¤ìš´ë¡œë“œí•´ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\níŒŒì¼ì„ ì—…ë¡œë“œí•œ ë’¤ ì‚¬ì´ë“œë°”ì—ì„œ ë¶„ì„í•  ì´ë²¤íŠ¸ IDì™€ ë¶„ì„ ê¸°ê°„(ê¸°íšì „ ì§„í–‰ê¸°ê°„)ì„ ì„ íƒí•˜ê³ , 'ë¶„ì„ ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.\nì´ í˜ì´ì§€ì— ë¬¸ì œê°€ ìƒê¸°ë©´ CRMíŒ€ **@ê¹€ì˜ˆìŠ¬** ì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.ğŸ€\n\n*í€µì‚¬ì´íŠ¸ì—ì„œ [ğŸ—“ï¸ëŒ€ìƒì¼ - descending]ì„ ì„ íƒí•´ì„œ CSVë¥¼ ë‹¤ìš´ë°›ëŠ” ê²ƒì„ ì¶”ì²œí•©ë‹ˆë‹¤. ì¡°íšŒ/ìƒë‹´ CSVì™€ CPV CSV ëª¨ë‘ ë¹„êµí•˜ê³ ì í•˜ëŠ” ë¶„ì„ ê¸°ê°„ì„ í¬í•¨í•˜ê³  ìˆì–´ì•¼í•©ë‹ˆë‹¤.
"""
)

uploaded_file = st.file_uploader(
    "ì¡°íšŒ/ìƒë‹´ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["csv"], key="primary_csv"
)
cpv_uploaded_file = st.file_uploader(
    "CPV CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=["csv"], key="cpv_csv"
)

if uploaded_file is None and cpv_uploaded_file is None:
    st.info("ì¡°íšŒ/ìƒë‹´ CSV ë˜ëŠ” CPV CSV ì¤‘ ìµœì†Œ í•˜ë‚˜ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

try:
    df_meta = (
        load_primary_meta(uploaded_file)
        if uploaded_file
        else pd.DataFrame(columns=["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)", "ì´ë²¤íŠ¸ ì´ë¦„", "ëŒ€ìƒì¼", "ë³‘ì› ì´ë¦„"])
    )
except ValueError as exc:
    st.error(str(exc))
    st.stop()
except Exception as exc:  # noqa: BLE001
    st.error(f"ì¡°íšŒ/ìƒë‹´ CSVë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}")
    st.stop()

try:
    cpv_meta = (
        load_cpv_meta(cpv_uploaded_file)
        if cpv_uploaded_file
        else pd.DataFrame(
            columns=[
                "ì´ë²¤íŠ¸ ID (ì‹ë³„ì)",
                "ì´ë²¤íŠ¸ ì´ë¦„",
                "ëŒ€ìƒì¼",
                "ë³‘ì› ì´ë¦„",
                "ëŒ€ì¹´í…Œê³ ë¦¬ëª…",
                "ì¤‘ì¹´í…Œê³ ë¦¬ëª…",
                "ì†Œì¹´í…Œê³ ë¦¬ëª…",
                "ì´ë²¤íŠ¸ í• ì¸ê°€",
            ]
        )
    )
except ValueError as exc:
    st.error(str(exc))
    st.stop()
except Exception as exc:  # noqa: BLE001
    st.error(f"CPV CSVë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}")
    st.stop()

combined_meta = safe_concat([df_meta, cpv_meta])
if combined_meta.empty:
    st.error("ì—…ë¡œë“œí•œ íŒŒì¼ì—ì„œ ì´ë²¤íŠ¸/ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    st.stop()

combined_meta = combined_meta.dropna(subset=["ëŒ€ìƒì¼"])
if combined_meta.empty:
    st.error("ë‚ ì§œ(ëŒ€ìƒì¼) ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

event_options = get_event_options(combined_meta)
if not event_options:
    st.error("ì´ë²¤íŠ¸ ì •ë³´ê°€ í¬í•¨ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

event_lookup = {event_id: event_name for event_id, event_name in event_options}
available_ids = list(event_lookup.keys())

min_date = combined_meta["ëŒ€ìƒì¼"].min().date()
max_date = combined_meta["ëŒ€ìƒì¼"].max().date()
st.sidebar.header("ë¶„ì„ ì„¤ì •")

default_event_input = available_ids[0] if available_ids else ""
if "analysis_params" not in st.session_state:
    st.session_state.analysis_params = None

with st.sidebar.form("analysis_form"):
    with st.expander("ì´ë²¤íŠ¸ ID ëª©ë¡ ë³´ê¸°"):
        st.dataframe(
            pd.DataFrame(event_options, columns=["ì´ë²¤íŠ¸ ID", "ì´ë²¤íŠ¸ ì´ë¦„"]).head(300),
            width="stretch",
        )
    event_input = st.text_area(
        "ë¶„ì„í•  ì´ë²¤íŠ¸ ID (ì¤„ë°”ê¿ˆ ë˜ëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„)",
        value=default_event_input,
        height=120,
        placeholder="ì˜ˆ: 53004\\n47917",
    )
    date_range = st.date_input(
        "ë¶„ì„ ê¸°ê°„ (ì§„í–‰ ê¸°ê°„)",
        value=(min_date, max_date),
        min_value=min_date,
        max_value=max_date,
    )
    submitted = st.form_submit_button("ë¶„ì„ ì‹œì‘", type="primary")

if submitted:
    start_date, end_date = (
        (date_range, date_range) if isinstance(date_range, date) else date_range
    )
    if start_date > end_date:
        st.sidebar.error("ì‹œì‘ì¼ì€ ì¢…ë£Œì¼ë³´ë‹¤ ì´ì „ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()
    selected_event_ids, invalid_event_ids = parse_event_ids_input(
        event_input,
        set(available_ids),
    )
    if invalid_event_ids:
        st.sidebar.error(
            f"ë‹¤ìŒ ì´ë²¤íŠ¸ IDëŠ” ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤: {', '.join(invalid_event_ids)}"
        )
        st.stop()
    if not selected_event_ids:
        st.sidebar.error("ìµœì†Œ í•œ ê°œì˜ ì´ë²¤íŠ¸ IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()
    st.session_state.analysis_params = {
        "selected_event_ids": selected_event_ids,
        "start_date": start_date,
        "end_date": end_date,
    }

if st.session_state.analysis_params is None:
    st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì´ë²¤íŠ¸/ê¸°ê°„ì„ ì„ íƒí•œ ë’¤ 'ë¶„ì„ ì‹œì‘'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
    st.stop()

selected_event_ids = st.session_state.analysis_params["selected_event_ids"]
start_date = st.session_state.analysis_params["start_date"]
end_date = st.session_state.analysis_params["end_date"]

try:
    df = (
        load_primary_data(uploaded_file)
        if uploaded_file
        else pd.DataFrame(columns=REQUIRED_COLUMNS)
    )
except ValueError as exc:
    st.error(str(exc))
    st.stop()
except Exception as exc:  # noqa: BLE001
    st.error(f"ì¡°íšŒ/ìƒë‹´ CSVë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}")
    st.stop()

try:
    cpv_df = (
        load_cpv_data(cpv_uploaded_file)
        if cpv_uploaded_file
        else pd.DataFrame(columns=CPV_REQUIRED_COLUMNS)
    )
except ValueError as exc:
    st.error(str(exc))
    st.stop()
except Exception as exc:  # noqa: BLE001
    st.error(f"CPV CSVë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}")
    st.stop()

if uploaded_file is None:
    st.warning("ì¡°íšŒ/ìƒë‹´ CSVê°€ ì—†ìœ¼ë©´ ì¡°íšŒ ìˆ˜ ë° ìƒë‹´ì‹ ì²­ ì§€í‘œëŠ” ì œê³µë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
if cpv_uploaded_file is None:
    st.info("CPV CSVë¥¼ ì—…ë¡œë“œí•˜ë©´ CPV ë§¤ì¶œ ë¶„ì„ì„ í•¨ê»˜ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

event_df = df[df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"].isin(selected_event_ids)].copy()
cpv_event_df = cpv_df[cpv_df["ì´ë²¤íŠ¸ ID (ì‹ë³„ì)"].isin(selected_event_ids)].copy()

if event_df.empty and cpv_event_df.empty:
    st.error("ì„ íƒí•œ ì´ë²¤íŠ¸ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

if event_df.empty:
    st.warning("ì„ íƒí•œ ì´ë²¤íŠ¸ì— ëŒ€í•œ ì¡°íšŒ/ìƒë‹´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
if cpv_event_df.empty:
    st.warning("ì„ íƒí•œ ì´ë²¤íŠ¸ì— ëŒ€í•œ CPV ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

current_start = pd.Timestamp(start_date)
current_end = pd.Timestamp(end_date)
period_days = (current_end - current_start).days + 1
if period_days <= 0:
    st.sidebar.error("ë¶„ì„ ê¸°ê°„ì€ ìµœì†Œ í•˜ë£¨ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    st.stop()

current_period_df = filter_period(event_df, current_start, current_end)
previous_end = current_start - timedelta(days=1)
previous_start = previous_end - timedelta(days=period_days - 1)
previous_period_df = filter_period(event_df, previous_start, previous_end)
cpv_current_period_df = filter_period(cpv_event_df, current_start, current_end)
cpv_previous_period_df = filter_period(cpv_event_df, previous_start, previous_end)

st.subheader(f"ì„ íƒëœ ì´ë²¤íŠ¸ ({len(selected_event_ids)}ê°œ)")
render_selected_events(selected_event_ids, event_lookup)
st.caption(
    f"ì§„í–‰ ê¸°ê°„: {current_start.date()} ~ {current_end.date()} | "
    f"ì´ì „ ê¸°ê°„: {previous_start.date()} ~ {previous_end.date()} "
    f"(ì´ {period_days}ì¼)"
)

if current_period_df.empty and cpv_current_period_df.empty:
    st.warning("ì„ íƒëœ ê¸°ê°„ ë‚´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ê¸°ê°„ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    st.stop()


def _metric_sum(df: pd.DataFrame, column: str) -> int:
    if df.empty or column not in df or df[column].dropna().empty:
        return None
    return int(df[column].sum())


metric_rows = [
    {
        "label": "ì¡°íšŒ ìˆ˜",
        "current": _metric_sum(current_period_df, "ì¡°íšŒ ìˆ˜"),
        "previous": _metric_sum(previous_period_df, "ì¡°íšŒ ìˆ˜")
        if not previous_period_df.empty
        else None,
    },
    {
        "label": "ìƒë‹´ì‹ ì²­ ìˆ˜",
        "current": _metric_sum(current_period_df, "ìƒë‹´ì‹ ì²­ ìˆ˜"),
        "previous": _metric_sum(previous_period_df, "ìƒë‹´ì‹ ì²­ ìˆ˜")
        if not previous_period_df.empty
        else None,
    },
    {
        "label": "CPV ì¡°íšŒ ìˆ˜",
        "current": _metric_sum(cpv_current_period_df, "CPV ì¡°íšŒ ìˆ˜"),
        "previous": _metric_sum(cpv_previous_period_df, "CPV ì¡°íšŒ ìˆ˜")
        if not cpv_previous_period_df.empty
        else None,
    },
    {
        "label": "CPV ë§¤ì¶œ",
        "current": _metric_sum(cpv_current_period_df, "CPV ë§¤ì¶œ"),
        "previous": _metric_sum(cpv_previous_period_df, "CPV ë§¤ì¶œ")
        if not cpv_previous_period_df.empty
        else None,
    },
]

render_metrics(metric_rows)

event_summary_df = build_event_summary(
    current_period_df,
    previous_period_df,
    cpv_current_period_df,
    cpv_previous_period_df,
    event_lookup,
)
event_insights = generate_event_insights(event_summary_df)
event_summary_display = format_event_summary_display(event_summary_df)

tab_insight, tab_trend = st.tabs(
    ["ğŸ’¡ ì´ë²¤íŠ¸ ì¸ì‚¬ì´íŠ¸", "ğŸ“ˆ ê¸°ê°„ë³„ ì¶”ì´"]
)

with tab_insight:
    st.markdown("#### ì´ë²¤íŠ¸ ì„±ê³¼ í•˜ì´ë¼ì´íŠ¸")
    render_insight_cards(event_insights)
    if not event_summary_df.empty:
        st.markdown("##### ì´ë²¤íŠ¸ë³„ ìƒì„¸ ì§€í‘œ")
        st.dataframe(
            event_summary_display,
            width="stretch",
            hide_index=True,
        )
        csv_bytes = event_summary_df.to_csv(index=False, encoding="utf-8-sig").encode(
            "utf-8-sig"
        )
        st.download_button(
            "ğŸ“¥ ì´ í…Œì´ë¸”ì„ CSVë¡œ ë‚´ë³´ë‚´ê¸° (click!)",
            data=csv_bytes,
            file_name="event_summary.csv",
            mime="text/csv",
            key="download_event_summary",
        )
    else:
        st.info("ì´ë²¤íŠ¸ë³„ ì„¸ë¶€ ì§€í‘œë¥¼ ê³„ì‚°í•  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

with tab_trend:
    st.markdown("#### ê¸°ê°„ë³„ ìƒë‹´ì‹ ì²­ ìˆ˜ ì¶”ì´")
    render_chart(current_period_df)

with st.expander("ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
    preview_df = event_df.sort_values("ëŒ€ìƒì¼").head(500)
    st.dataframe(preview_df, width="stretch", hide_index=True)
